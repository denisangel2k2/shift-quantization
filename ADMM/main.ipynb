{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7a9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the ADMMQuantizer class\n",
    "from admm2 import ADMMQuantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d8054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions (train_epoch, evaluate_model) ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, admm_quantizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        base_loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Apply ADMM penalty to the loss\n",
    "        if admm_quantizer is not None:\n",
    "            augmented_loss = admm_quantizer.apply_loss_penalty(base_loss)\n",
    "        else:\n",
    "            augmented_loss = base_loss\n",
    "        \n",
    "        augmented_loss.backward()\n",
    "\n",
    "        # Manual gradient injection for ADMM proximal update\n",
    "        if admm_quantizer is not None:\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in admm_quantizer.quantize_layers and param.grad is not None:\n",
    "                    z = admm_quantizer.Z[name]\n",
    "                    u = admm_quantizer.U[name]\n",
    "                    update_term = admm_quantizer.rho * (param.data - z + u).view_as(param)\n",
    "                    param.grad.data.add_(update_term)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += augmented_loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce0951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3f6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_final_quantization(model, admm_quantizer):\n",
    "    \"\"\"\n",
    "    Replaces the full-precision weights in the model with their quantized Z counterparts\n",
    "    for inference.\n",
    "    \"\"\"\n",
    "    print(\"Applying final quantization to model weights...\")\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in admm_quantizer.quantize_layers:\n",
    "                # Copy the quantized Z values into the model's parameters\n",
    "                param.copy_(admm_quantizer.Z[name].view_as(param))\n",
    "    print(\"Model weights permanently quantized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62ee0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Layers selected for ADMM quantization: ['conv1.weight', 'layer1.0.conv1.weight', 'layer1.0.conv2.weight', 'layer1.1.conv1.weight', 'layer1.1.conv2.weight', 'layer2.0.conv1.weight', 'layer2.0.conv2.weight', 'layer2.0.downsample.0.weight', 'layer2.1.conv1.weight', 'layer2.1.conv2.weight', 'layer3.0.conv1.weight', 'layer3.0.conv2.weight', 'layer3.0.downsample.0.weight', 'layer3.1.conv1.weight', 'layer3.1.conv2.weight', 'layer4.0.conv1.weight', 'layer4.0.conv2.weight', 'layer4.0.downsample.0.weight', 'layer4.1.conv1.weight', 'layer4.1.conv2.weight', 'fc.weight']\n",
      "Warmup: Training model in float32 before quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [03:20<00:00,  4.67it/s]\n",
      "Training: 100%|██████████| 938/938 [03:21<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initial Evaluation (Full Precision) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Loss: 0.0142, Initial Test Acc: 0.9952\n",
      "\n",
      "--- ADMM Outer Iteration 1/3 ---\n",
      "--- Fine-tuning for 1 epochs with ADMM penalty ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [03:25<00:00,  4.57it/s]\n",
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMM Iter 1, Epoch 1/1 - Train Loss: 0.2770, Train Acc: 0.9973 | Test Loss: 0.0148, Test Acc: 0.9947\n",
      "Z and U updated after ADMM Outer Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after ADMM Outer Iter 1 (W still FP32): 0.9947\n",
      "\n",
      "--- ADMM Outer Iteration 2/3 ---\n",
      "--- Fine-tuning for 1 epochs with ADMM penalty ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [03:25<00:00,  4.56it/s]\n",
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMM Iter 2, Epoch 1/1 - Train Loss: 0.7352, Train Acc: 0.9983 | Test Loss: 0.0177, Test Acc: 0.9937\n",
      "Z and U updated after ADMM Outer Iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after ADMM Outer Iter 2 (W still FP32): 0.9937\n",
      "\n",
      "--- ADMM Outer Iteration 3/3 ---\n",
      "--- Fine-tuning for 1 epochs with ADMM penalty ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 938/938 [03:25<00:00,  4.56it/s]\n",
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADMM Iter 3, Epoch 1/1 - Train Loss: 1.2594, Train Acc: 0.9986 | Test Loss: 0.0156, Test Acc: 0.9950\n",
      "Z and U updated after ADMM Outer Iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after ADMM Outer Iter 3 (W still FP32): 0.9950\n",
      "\n",
      "--- ADMM Training Completed ---\n",
      "Applying final quantization to model weights...\n",
      "Model weights permanently quantized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 79/79 [00:15<00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Quantized Model Test Loss: 0.2718, Final Quantized Model Test Acc: 0.9034\n",
      "Effective bit width: 2.0257833330739796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Data Loading and Preprocessing for MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1)), # Convert 1-channel to 3-channel\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST normalization (standard values)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 2. Model Initialization (ResNet-18)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10) # 10 classes for MNIST\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. ADMM Configuration\n",
    "# Identify layers to quantize: typically Conv2d and Linear layers' weights\n",
    "quantize_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        if hasattr(module, 'weight'):\n",
    "            quantize_layers.append(f\"{name}.weight\")\n",
    "\n",
    "print(f\"Layers selected for ADMM quantization: {quantize_layers}\")\n",
    "\n",
    "initial_lr = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Phase 1: Warmup training (no ADMM) ---\n",
    "print(\"Warmup: Training model in float32 before quantization...\")\n",
    "for epoch in range(2):\n",
    "    train_epoch(model, train_loader, optimizer, criterion, admm_quantizer=None, device=device)\n",
    "\n",
    "admm_rho = 1e-3 # Initial ADMM penalty parameter\n",
    "admm_target_bits = 2 # For ternary quantization\n",
    "admm_quantizer = ADMMQuantizer(model, quantize_layers, rho=admm_rho, target_bits=admm_target_bits)\n",
    "\n",
    "num_admm_iterations = 3 # Number of ADMM outer iterations (where Z, U are updated)\n",
    "epochs_per_admm_iter = 1 # Number of gradient descent epochs for W-update per ADMM iter\n",
    "full_precision_bits = 32 # For float32\n",
    "\n",
    "# Lists to store results for plotting\n",
    "admm_iter_labels = [\"Initial (FP32)\"] + [f\"ADMM Iter {i+1}\" for i in range(num_admm_iterations)] + [\"Final Quantized\"]\n",
    "accuracy_history = []\n",
    "effective_bits_history = []\n",
    "\n",
    "# Initial evaluation (Full Precision)\n",
    "print(\"\\n--- Initial Evaluation (Full Precision) ---\")\n",
    "loss, acc = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Initial Test Loss: {loss:.4f}, Initial Test Acc: {acc:.4f}\")\n",
    "accuracy_history.append(acc)\n",
    "effective_bits_history.append(full_precision_bits) # Model is initially full precision\n",
    "\n",
    "# ADMM Training Loop\n",
    "for admm_iter_idx in range(num_admm_iterations):\n",
    "    print(f\"\\n--- ADMM Outer Iteration {admm_iter_idx + 1}/{num_admm_iterations} ---\")\n",
    "\n",
    "    # Step 1: W-update (Train with augmented loss)\n",
    "    print(f\"--- Fine-tuning for {epochs_per_admm_iter} epochs with ADMM penalty ---\")\n",
    "    for epoch in range(epochs_per_admm_iter):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, admm_quantizer, device)\n",
    "        test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "        print(f\"ADMM Iter {admm_iter_idx+1}, Epoch {epoch+1}/{epochs_per_admm_iter} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    # Step 2 & 3: Z-update and U-update\n",
    "    admm_quantizer.step()\n",
    "    print(f\"Z and U updated after ADMM Outer Iteration {admm_iter_idx + 1}\")\n",
    "\n",
    "    # Evaluate model after W, Z, U updates (still conceptually full-precision W)\n",
    "    _, current_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"Test Accuracy after ADMM Outer Iter {admm_iter_idx+1} (W still FP32): {current_acc:.4f}\")\n",
    "    accuracy_history.append(current_acc)\n",
    "    # During ADMM training, the model weights are still full-precision,\n",
    "    # so effective bit-width remains 32 bits for the `param.data`.\n",
    "    effective_bits_history.append(full_precision_bits)\n",
    "\n",
    "\n",
    "print(\"\\n--- ADMM Training Completed ---\")\n",
    "\n",
    "# Final Evaluation of the fully quantized model\n",
    "# Apply the final quantization by copying Z to model weights\n",
    "apply_final_quantization(model, admm_quantizer)\n",
    "\n",
    "# Evaluate the truly quantized model\n",
    "final_quantized_loss, final_quantized_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Final Quantized Model Test Loss: {final_quantized_loss:.4f}, Final Quantized Model Test Acc: {final_quantized_acc:.4f}\")\n",
    "print(f\"Effective bit width: {admm_quantizer.calculate_effective_bit_width_admm(full_precision_bits)}\")\n",
    "# Append the final quantized model's accuracy and its target bit-width\n",
    "accuracy_history.append(final_quantized_acc)\n",
    "effective_bits_history.append(admm_quantizer.target_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9756a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Plotting Results ---\n",
      "Debug: len(admm_iter_labels)=5\n",
      "Debug: len(accuracy_history)=5\n",
      "Debug: len(effective_bits_history)=5\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- Plotting the results ---\n",
    "print(\"\\n--- Plotting Results ---\")\n",
    "\n",
    "# Ensure labels match data points\n",
    "print(f\"Debug: len(admm_iter_labels)={len(admm_iter_labels)}\")\n",
    "print(f\"Debug: len(accuracy_history)={len(accuracy_history)}\")\n",
    "print(f\"Debug: len(effective_bits_history)={len(effective_bits_history)}\")\n",
    "\n",
    "# The labels should match 1 (initial) + num_admm_iterations (intermediate) + 1 (final quantized)\n",
    "expected_len = 1 + num_admm_iterations + 1\n",
    "assert len(admm_iter_labels) == expected_len and \\\n",
    "        len(accuracy_history) == expected_len and \\\n",
    "        len(effective_bits_history) == expected_len, \\\n",
    "        f\"Mismatch in lengths: labels={len(admm_iter_labels)}, acc={len(accuracy_history)}, bits={len(effective_bits_history)}. Expected: {expected_len}\"\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('ADMM Stage')\n",
    "ax1.set_ylabel('Test Accuracy', color=color)\n",
    "ax1.plot(admm_iter_labels, accuracy_history, marker='o', linestyle='-', color=color, label='Test Accuracy')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim(0.0, 1.0) # Accuracy range\n",
    "\n",
    "ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Effective Bit-width', color=color)\n",
    "ax2.plot(admm_iter_labels, effective_bits_history, marker='x', linestyle='--', color=color, label='Effective Bit-width')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "# Set y-axis limits for bit-width. Max is 32, min is `target_bits`\n",
    "ax2.set_ylim(admm_target_bits - 1, full_precision_bits + 1) \n",
    "\n",
    "fig.suptitle('ADMM Quantization Performance: Accuracy vs. Effective Bit-width')\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to make room for suptitle\n",
    "\n",
    "# Add legends\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='lower right')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45, ha=\"right\") # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
